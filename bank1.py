import numpy as np
import pandas as pd
# import matplotlib.pyplot as plt # Non utilis√© directement dans ce code, peut √™tre enlev√© si inutile ailleurs
# import seaborn as sns # Non utilis√© directement dans ce code, peut √™tre enlev√© si inutile ailleurs
import streamlit as st
import altair as alt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import LabelEncoder
from sklearn.utils import resample
import pickle
import base64 # Utile pour une alternative d'encodage si l'URL pose probl√®me

# --- Configuration de la page ---
st.set_page_config(
    page_title="Classification des Donn√©es Bancaires",
    page_icon="üè¶",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- Fonction pour ajouter l'arri√®re-plan ---
def add_bg_from_url(url):
    """Ajoute une image d'arri√®re-plan √† partir d'une URL."""
    st.markdown(
         f"""
         <style>
         .stApp {{
             background-image: url("{url}");
             background-attachment: fixed;
             background-size: cover;
             background-repeat: no-repeat;
         }}
         /* Ajout d'un peu de transparence aux √©l√©ments principaux pour mieux voir le fond */
         /* [data-testid="stSidebar"], [data-testid="stHeader"], .main .block-container {{
             background-color: rgba(255, 255, 255, 0.8); /* Blanc avec 80% opacit√© */
             /* Vous pouvez ajuster la couleur et l'opacit√© */
         /* }} */
         /* Optionnel: Style pour rendre le texte plus lisible sur l'image */
         /* body, .stMarkdown, .stButton > button, .stTextInput > div > div > input, .stNumberInput > div > div > input {{
             color: #FFFFFF; /* Texte blanc */
             /* text-shadow: 1px 1px 2px black; /* Ombre port√©e pour lisibilit√© */
         /* }} */

         </style>
         """,
         unsafe_allow_html=True
     )

# --- URL de l'image brute sur GitHub ---
image_url = "https://raw.githubusercontent.com/teguegni/bank-additionnal-full/main/image1.jpg"
add_bg_from_url(image_url)


# --- Th√®me Altair ---
# alt.themes.enable("dark") # Peut entrer en conflit avec un fond clair, √† ajuster si besoin

# --- Barre lat√©rale ---
if 'page_selection' not in st.session_state:
    st.session_state.page_selection = 'a_propos'  # Page par d√©faut

# Fonction pour mettre √† jour page_selection
def set_page_selection(page):
    st.session_state.page_selection = page

with st.sidebar:
    st.title('üè¶ Classification Donn√©es Bancaires')
    # Navigation par boutons
    st.subheader("Sections")
    st.button("√Ä Propos", use_container_width=True, on_click=set_page_selection, args=('a_propos',), key="btn_a_propos")
    st.button("Jeu de Donn√©es", use_container_width=True, on_click=set_page_selection, args=('jeu_de_donnees',), key="btn_jeu_de_donnees")
    st.button("Analyse Exploratoire", use_container_width=True, on_click=set_page_selection, args=('analyse_exploratoire',), key="btn_analyse_exploratoire")
    # Correction: Ajout du bouton pour la section 'apprentissage_automatique' si elle existe
    st.button("Apprentissage Automatique", use_container_width=True, on_click=set_page_selection, args=('apprentissage_automatique',), key="btn_apprentissage")
    st.button("Pr√©diction", use_container_width=True, on_click=set_page_selection, args=('prediction',), key="btn_prediction")
    st.button("Conclusion", use_container_width=True, on_click=set_page_selection, args=('conclusion',), key="btn_conclusion")

    # D√©tails du projet
    st.subheader("R√©sum√©")
    st.markdown("""
        Un tableau de bord interactif pour explorer et classifier les donn√©es d'une campagne marketing bancaire.
        - [Jeu de Donn√©es](URL_VERS_VOTRE_JEU_DE_DONNEES) - [Notebook Google Colab](URL_VERS_VOTRE_NOTEBOOK) - [D√©p√¥t GitHub](https://github.com/teguegni/bank-additionnal-full) Auteur : Kenfack Teguegni Junior
    """)

# --- Chargement des donn√©es ---
# Utiliser un chemin relatif ou absolu correct, ou s'assurer que le fichier est dans le m√™me dossier
DATA_URL = 'bank-additional-full.csv'
try:
    df_original = pd.read_csv(DATA_URL, delimiter=';')
    # Cr√©er une copie pour les manipulations afin de ne pas modifier l'original √† chaque rechargement
    df = df_original.copy()
except FileNotFoundError:
    st.error(f"Le fichier '{DATA_URL}' est introuvable. Assurez-vous qu'il se trouve dans le bon r√©pertoire.")
    # Essayer de charger depuis l'URL brute du d√©p√¥t GitHub comme alternative
    try:
        st.warning("Tentative de chargement du fichier depuis GitHub...")
        github_data_url = "https://raw.githubusercontent.com/teguegni/bank-additionnal-full/main/bank-additional-full.csv"
        df_original = pd.read_csv(github_data_url, delimiter=';')
        df = df_original.copy()
        st.success("Chargement depuis GitHub r√©ussi.")
    except Exception as e:
        st.error(f"√âchec du chargement depuis GitHub √©galement. Erreur : {e}")
        st.stop() # Arr√™ter l'ex√©cution si les donn√©es ne peuvent pas √™tre charg√©es


# --- Fonction principale et logique des pages ---
def main():
    # Page principale
    if st.session_state.page_selection == 'a_propos':
        st.title("Ô∏è √Ä Propos")
        st.markdown("""
            Cette application explore le jeu de donn√©es Bank Marketing et propose :
            - Une exploration visuelle des donn√©es.
            - Un pr√©traitement et nettoyage des donn√©es.
            - La construction et l'√©valuation de mod√®les d'apprentissage automatique.
            - Une interface interactive pour pr√©dire si un client souscrira √† un produit.

            **Technologies utilis√©es :**
            - Python (Streamlit, Altair, Pandas, Scikit-learn)
            - Machine Learning (Random Forest)

            **Auteur :** Kenfack Teguegni Junior
            ‚úâÔ∏è **Contact :** kenfackteguegni@gmail.com
            """)

    elif st.session_state.page_selection == 'conclusion':
        st.title("Ô∏è Conclusion")
        st.markdown("""
            Un traitement minutieux et r√©fl√©chi du DataFrame `bank-additional-full.csv` est fondamental pour maximiser la pr√©cision
            et la fiabilit√© du mod√®le de pr√©diction. En combinant explorations, pr√©traitements ad√©quats (gestion des 'unknown', encodage, traitement des outliers, r√©√©quilibrage), et √©valuations rigoureuses,
            un mod√®le robuste peut √™tre d√©velopp√© pour mieux pr√©dire les comportements des clients envers la souscription √† un produit.

            Ce tableau de bord Streamlit fournit une interface interactive pour visualiser les donn√©es, comprendre les √©tapes de pr√©traitement, et tester le mod√®le de pr√©diction final.
            """)

    elif st.session_state.page_selection == 'jeu_de_donnees':
        st.title(" Jeu de Donn√©es")
        st.markdown("Aper√ßu du jeu de donn√©es brut charg√©.")
        # Afficher les premi√®res lignes du DataFrame original
        if st.checkbox("Afficher le DataFrame brut", key="chk_afficher_df_brut", value=True):
            nb_rows = st.slider("Nombre de lignes √† afficher :", min_value=5, max_value=50, value=10, key="slider_nb_rows_brut")
            st.dataframe(df_original.head(nb_rows)) # Utiliser df_original ici

        st.markdown("---")
        st.markdown("Informations sur le DataFrame :")
        st.write(f"Nombre de lignes : {df_original.shape[0]}")
        st.write(f"Nombre de colonnes : {df_original.shape[1]}")

        # Afficher les statistiques descriptives
        if st.checkbox("Afficher les statistiques descriptives (Num√©riques)", key="chk_stats_desc_num"):
            st.write(df_original.describe(include=np.number))
        if st.checkbox("Afficher les statistiques descriptives (Cat√©gorielles)", key="chk_stats_desc_cat"):
             st.write(df_original.describe(include='object'))


    elif st.session_state.page_selection == 'analyse_exploratoire':
        st.title(" Analyse Exploratoire et Pr√©traitement")

        # Utiliser la copie df pour les modifications
        df_processed = df.copy()

        st.subheader("1. Gestion des Duplications")
        duplicates_before = df_processed.duplicated().sum()
        st.write(f"Nombre de lignes dupliqu√©es avant suppression : {duplicates_before}")
        if duplicates_before > 0:
            df_processed = df_processed.drop_duplicates()
            st.write(f"Nombre de lignes apr√®s suppression des duplications : {df_processed.shape[0]}")
        else:
            st.write("Aucune ligne dupliqu√©e trouv√©e.")

        st.subheader("2. Gestion des Valeurs 'unknown'")
        st.write("Remplacement des valeurs 'unknown' par le mode (valeur la plus fr√©quente) de chaque colonne cat√©gorielle.")
        unknown_counts = {}
        for column in df_processed.select_dtypes(include='object').columns:
            if 'unknown' in df_processed[column].unique():
                unknown_counts[column] = df_processed[column].value_counts().get('unknown', 0)
                mode_value = df_processed[column].mode()[0]
                if mode_value == 'unknown': # Si le mode lui-m√™me est 'unknown', choisir le second mode si possible
                   modes = df_processed[column].mode()
                   if len(modes) > 1:
                       mode_value = modes[1]
                   else: # Cas tr√®s improbable o√π 'unknown' est la seule valeur
                       st.warning(f"La seule valeur dans '{column}' est 'unknown'. Impossible de remplacer.")
                       continue # Passer √† la colonne suivante
                df_processed[column] = df_processed[column].replace('unknown', mode_value)
        if unknown_counts:
             st.write("Nombre de valeurs 'unknown' remplac√©es par colonne :")
             st.write(unknown_counts)
        else:
             st.write("Aucune valeur 'unknown' trouv√©e dans les colonnes objectives.")


        st.subheader("3. V√©rification des Valeurs Manquantes (apr√®s remplacement des 'unknown')")
        missing_values = df_processed.isnull().sum()
        missing_data = missing_values[missing_values > 0]
        if not missing_data.empty:
            st.write("Colonnes avec des valeurs manquantes restantes :")
            st.write(missing_data)
            # Ici, vous pourriez ajouter une strat√©gie pour imputer ces valeurs si n√©cessaire
        else:
            st.write("Aucune valeur manquante (null) d√©tect√©e apr√®s le traitement des 'unknown'.")


        st.subheader("4. Visualisation Exploratoire")
        st.write("Relation entre l'√¢ge, le m√©tier et la souscription (variable cible 'y').")

        # S'assurer que 'job' est de type object ou category pour le graphique
        df_processed['job'] = df_processed['job'].astype('category')

        # Utiliser une copie pour √©viter les avertissements de modification
        df_chart = df_processed.copy()
        df_chart['Souscription'] = df_chart['y'].map({'yes': 'Oui', 'no': 'Non'}) # Rendre la l√©gende plus claire


        age_job_chart = (
            alt.Chart(df_chart)
            .mark_circle(size=60, opacity=0.7) # Ajout d'opacit√© pour les points superpos√©s
            .encode(
                x=alt.X('age', title='√Çge'),
                y=alt.Y('job', title='M√©tier', sort='-x'), # Trier les m√©tiers par exemple
                color=alt.Color('Souscription', title='Souscrit ?', scale=alt.Scale(scheme='category10')), # Utiliser la colonne renomm√©e
                tooltip=[
                    alt.Tooltip('age', title='√Çge'),
                    alt.Tooltip('job', title='M√©tier'),
                    alt.Tooltip('Souscription', title='Souscrit ?')
                    ]
            )
            .properties(
                title="Relation √Çge / M√©tier color√©e par la Souscription",
                # width=700, # Laisser Streamlit g√©rer la largeur avec use_container_width
                height=500
            )
            .interactive() # Permet le zoom et le d√©placement
        )
        st.altair_chart(age_job_chart, use_container_width=True)

        # Sauvegarder l'√©tat trait√© pour la page suivante si n√©cessaire
        # st.session_state.df_processed = df_processed # D√©commenter si besoin


    elif st.session_state.page_selection == 'apprentissage_automatique':
        st.title("‚öôÔ∏è Apprentissage Automatique")
        st.write("Pr√©paration des donn√©es, entra√Ænement et √©valuation du mod√®le Random Forest.")

        # R√©cup√©rer le df trait√© de l'√©tape pr√©c√©dente ou le recalculer
        # Pour la simplicit√© ici, on refait les √©tapes de pr√©traitement minimales n√©cessaires
        # Assurez-vous que les √©tapes ici correspondent √† celles de 'analyse_exploratoire'
        df_ml = df_original.copy() # Partir de l'original pour cette section isol√©e

        # 1. Doublons
        df_ml = df_ml.drop_duplicates()

        # 2. 'unknown'
        for column in df_ml.select_dtypes(include='object').columns:
             if 'unknown' in df_ml[column].unique():
                mode_value = df_ml[column].mode()[0]
                if mode_value == 'unknown':
                   modes = df_ml[column].mode()
                   if len(modes) > 1: mode_value = modes[1]
                   else: continue
                df_ml[column] = df_ml[column].replace('unknown', mode_value)

        # 3. Outliers (Optionnel, mais pr√©sent dans le code original)
        st.subheader("Traitement des Valeurs Aberrantes (Outliers)")
        st.write("Remplacement des outliers par les limites bas√©es sur l'IQR (Interquartile Range).")
        numerics = df_ml.select_dtypes(include=np.number).columns.tolist()
        if st.checkbox("Activer le remplacement des outliers", key="cb_outliers", value=True):
            df_ml_outliers = df_ml.copy() # Travailler sur une copie
            outliers_replaced_count = {col: 0 for col in numerics}
            for col in numerics:
                Q1 = df_ml_outliers[col].quantile(0.25)
                Q3 = df_ml_outliers[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR

                original_values = df_ml_outliers[col].copy()
                df_ml_outliers[col] = np.where(df_ml_outliers[col] < lower_bound, lower_bound, df_ml_outliers[col])
                df_ml_outliers[col] = np.where(df_ml_outliers[col] > upper_bound, upper_bound, df_ml_outliers[col])

                # Compter combien de valeurs ont √©t√© modifi√©es
                outliers_replaced_count[col] = (original_values != df_ml_outliers[col]).sum()

            st.write("Nombre d'outliers remplac√©s par colonne num√©rique :")
            st.write({k: v for k, v in outliers_replaced_count.items() if v > 0})
            df_ml = df_ml_outliers # Utiliser la version trait√©e pour la suite
        else:
            st.write("Remplacement des outliers d√©sactiv√©.")


        st.subheader("Encodage des Variables Cat√©gorielles")
        # Encodage par fr√©quence pour certaines colonnes (comme dans le code original)
        st.write("Encodage par fr√©quence pour : 'marital', 'job', 'education', 'month', 'day_of_week', 'poutcome'")
        df_encoded = df_ml.copy()
        categorical_cols_freq = ['marital', 'job', 'education', 'month', 'day_of_week', 'poutcome']
        for column in categorical_cols_freq:
            fe = df_encoded.groupby(column).size() / len(df_encoded)
            df_encoded[f'{column}_freq_encode'] = df_encoded[column].map(fe)

        # Encodage Label pour les binaires/ordinales simples (comme dans le code original)
        st.write("Encodage Label (0/1) pour : 'default', 'housing', 'loan', 'contact'")
        le = LabelEncoder()
        categorical_cols_label = ['default', 'housing', 'loan', 'contact']
        for column in categorical_cols_label:
            # G√©rer le cas o√π 'yes'/'no' sont pr√©sents mais peut-√™tre pas d'autres valeurs
            try:
                 df_encoded[column] = le.fit_transform(df_encoded[column])
            except Exception as e:
                 st.error(f"Erreur d'encodage pour {column}: {e}")
                 st.write(f"Valeurs uniques dans {column}: {df_encoded[column].unique()}")


        # Encodage de la colonne cible 'y'
        st.write("Encodage Label (0/1) pour la variable cible 'y' ('no'=0, 'yes'=1)")
        encoder_y = LabelEncoder()
        # S'assurer que 'y' existe avant d'encoder
        if 'y' in df_encoded.columns:
            df_encoded['y_encoded'] = encoder_y.fit_transform(df_encoded['y'])
            target_mapping = dict(zip(encoder_y.classes_, encoder_y.transform(encoder_y.classes_)))
            st.write(f"Mapping de la cible : {target_mapping}")
        else:
            st.error("La colonne cible 'y' est manquante.")
            st.stop()

        # Suppression des colonnes cat√©gorielles originales et de 'y'
        cols_to_drop = categorical_cols_freq + categorical_cols_label + ['y']
        # V√©rifier que les colonnes √† supprimer existent bien
        cols_to_drop_existing = [col for col in cols_to_drop if col in df_encoded.columns]
        df_final = df_encoded.drop(columns=cols_to_drop_existing)

        st.subheader("Pr√©paration Finale des Donn√©es")
        st.write("Dimensions du DataFrame pr√™t pour le mod√®le : ", df_final.shape)
        if st.checkbox("Afficher les premi√®res lignes des donn√©es encod√©es", key="chk_df_final"):
             st.dataframe(df_final.head())


        st.subheader("Gestion du D√©s√©quilibre des Classes")
        target_counts = df_final['y_encoded'].value_counts(normalize=True)
        st.write("Distribution de la classe cible avant r√©√©quilibrage :")
        st.write(target_counts)

        if st.checkbox("Activer le sur√©chantillonnage (Upsampling) de la classe minoritaire", key="cb_upsampling", value=True):
             st.write("Application du sur√©chantillonnage pour √©quilibrer les classes...")
             df_majority = df_final[df_final.y_encoded == 0]
             df_minority = df_final[df_final.y_encoded == 1]

             if len(df_minority) == 0 or len(df_majority) == 0:
                 st.error("Impossible de sur√©chantillonner : une des classes est vide.")
                 st.stop()

             df_minority_upsampled = resample(df_minority,
                                              replace=True,     # √©chantillonnage avec remplacement
                                              n_samples=len(df_majority), # pour correspondre √† la classe majoritaire
                                              random_state=42) # pour la reproductibilit√©

             df_upsampled = pd.concat([df_majority, df_minority_upsampled])
             st.write("Distribution de la classe cible apr√®s sur√©chantillonnage :")
             st.write(df_upsampled['y_encoded'].value_counts(normalize=True))
             data_ready = df_upsampled
        else:
             st.write("Entra√Ænement sur les donn√©es d√©s√©quilibr√©es.")
             data_ready = df_final


        st.subheader("Division Entra√Ænement / Test")
        X = data_ready.drop(columns=['y_encoded']).values
        y = data_ready['y_encoded'].values
        test_size = st.slider("Pourcentage pour l'ensemble de test", 0.1, 0.5, 0.2, 0.05, key="slider_test_size")
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42, stratify=y) # stratify est important, surtout si non r√©√©quilibr√©
        st.write(f"Taille de l'ensemble d'entra√Ænement : {X_train.shape[0]} √©chantillons")
        st.write(f"Taille de l'ensemble de test : {X_test.shape[0]} √©chantillons")


        st.subheader("Entra√Ænement du Mod√®le Random Forest")
        n_estimators = st.slider("Nombre d'arbres (n_estimators)", 50, 500, 100, 50, key="slider_n_estimators")
        max_depth = st.select_slider("Profondeur maximale (max_depth)", options=[None, 5, 10, 15, 20, 30], value=None, key="slider_max_depth") # None = pas de limite

        with st.spinner("Entra√Ænement du mod√®le en cours..."):
             model = RandomForestClassifier(n_estimators=n_estimators,
                                           max_depth=max_depth,
                                           random_state=42,
                                           n_jobs=-1) # Utiliser tous les c≈ìurs CPU
             model.fit(X_train, y_train)
        st.success("Mod√®le entra√Æn√© avec succ√®s !")


        st.subheader("√âvaluation du Mod√®le")
        y_pred = model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        report = classification_report(y_test, y_pred, target_names=encoder_y.classes_) # Utiliser les noms originaux

        st.metric(label="Accuracy sur l'ensemble de test", value=f"{accuracy:.2%}")

        st.text('Rapport de Classification :')
        st.text(report) # Utiliser st.text pour pr√©server le formatage


        st.subheader("Sauvegarder le Mod√®le Entra√Æn√©")
        model_filename = 'model_bank_data.pkl'
        if st.button("Sauvegarder le mod√®le", key="btn_save_model"):
             try:
                with open(model_filename, 'wb') as model_file:
                    # Sauvegarder le mod√®le, l'encodeur de la cible, et les colonnes attendues
                    pickle.dump({
                        'model': model,
                        'encoder_y': encoder_y,
                        'features': data_ready.drop(columns=['y_encoded']).columns.tolist() # Important pour la pr√©diction
                    }, model_file)
                st.success(f"Mod√®le sauvegard√© sous le nom '{model_filename}'")
             except Exception as e:
                st.error(f"Erreur lors de la sauvegarde du mod√®le : {e}")


    elif st.session_state.page_selection == 'prediction':
    st.title("üîÆ Pr√©diction de Souscription Client")

    # --- Chargement du mod√®le depuis GitHub (comme avant) ---
    MODEL_URL = "https://raw.githubusercontent.com/teguegni/bank-additionnal-full/main/model_classification_bank.pkl"
    model = None
    encoder_y = None
    expected_features = None

    @st.cache_resource
    def load_model_from_github(url):
        # ... (code de la fonction load_model_from_github comme avant) ...
        try:
            st.info(f"T√©l√©chargement et chargement du mod√®le depuis {url}...")
            response = requests.get(url, timeout=30)
            response.raise_for_status()
            saved_data = pickle.loads(response.content)
            model_loaded = saved_data.get('model')
            encoder_loaded = saved_data.get('encoder_y')
            features_loaded = saved_data.get('features')
            if not all([model_loaded, encoder_loaded, features_loaded]):
                 st.error("Le fichier pickle charg√© ne contient pas toutes les cl√©s attendues ('model', 'encoder_y', 'features').")
                 return None, None, None
            st.success("Mod√®le charg√© avec succ√®s depuis GitHub.")
            return model_loaded, encoder_loaded, features_loaded
        except requests.exceptions.RequestException as e:
            st.error(f"Erreur r√©seau lors du t√©l√©chargement du mod√®le : {e}")
            return None, None, None
        except pickle.UnpicklingError as e:
            st.error(f"Erreur lors du d√©s√©rialisage du fichier pickle : {e}")
            return None, None, None
        except Exception as e:
            st.error(f"Une erreur inattendue est survenue lors du chargement du mod√®le : {e}")
            return None, None, None

    model, encoder_y, expected_features = load_model_from_github(MODEL_URL)

    if model is None or encoder_y is None or expected_features is None:
        st.error("Impossible de continuer sans le mod√®le charg√©.")
        st.stop()

    # Essayer de s'assurer que df_original est disponible ici si n√©cessaire pour les options des selectbox
    # Cela suppose que df_original est charg√© plus t√¥t dans le script globalement ou dans la session state
    # Si ce n'est pas le cas, vous devrez g√©rer le chargement de df_original ici ou utiliser des listes statiques
    df_original = None # Initialiser
    try:
        # Supposons que df est charg√© plus t√¥t et est accessible (sinon, il faut le charger ici)
        # Remplacer 'df' par la variable r√©elle contenant le dataframe original si elle a un autre nom
        if 'df' in locals() or 'df' in globals():
            # Assurez-vous que 'df' est bien le dataframe *original* non modifi√©
            df_original = df.copy() # Utiliser une copie pour √™tre s√ªr
        else:
            # Tentative de rechargement si non trouv√© (ajuster le chemin/URL si n√©cessaire)
            st.warning("df_original non trouv√©, tentative de rechargement...")
            DATA_URL = 'bank-additional-full.csv' # Ou l'URL GitHub
            try:
                 df_original = pd.read_csv(DATA_URL, delimiter=';')
            except FileNotFoundError:
                 github_data_url = "https://raw.githubusercontent.com/teguegni/bank-additionnal-full/main/bank-additional-full.csv"
                 df_original = pd.read_csv(github_data_url, delimiter=';')
            st.success("df_original recharg√©.")

    except Exception as e:
        st.error(f"Impossible de charger df_original pour les options du formulaire : {e}")
        # On pourrait continuer avec des listes par d√©faut, mais l'encodage √©chouera probablement plus tard

    st.markdown("Entrez les informations du client...")

    with st.form(key='prediction_form'):
        # ... (d√©finition du formulaire comme avant, utilisant df_original pour les options si possible) ...
        st.subheader("Informations du Client")
        col1, col2, col3 = st.columns(3)
        # --- Champs de saisie (comme avant, mais s'assurer que les listes d'options sont d√©finies) ---
        # (Exemple pour job)
        try: job_options = df_original['job'].unique().tolist() if df_original is not None else ['admin.', 'technician', 'blue-collar'] # Fallback
        except: job_options = ['admin.', 'technician', 'blue-collar'] # Fallback plus s√ªr
        job = st.selectbox("M√©tier", options=job_options, index=0, key="input_job")
        # ... (R√©p√©ter pour marital, education, default, housing, loan, contact, month, day_of_week, poutcome en utilisant df_original si possible avec fallback)

        # ... Autres champs (age, duration, etc.) ...

        st.subheader("Indicateurs √âconomiques")
        # ... (Champs indicateurs √©conomiques comme avant) ...

        submitted = st.form_submit_button("üîÆ Lancer la Pr√©diction")

        if submitted:
            # --- Cr√©ation du DataFrame d'entr√©e (comme avant) ---
            input_dict = { ... } # Remplir avec les valeurs du formulaire
            input_df = pd.DataFrame([input_dict])

            # --- D√©finition des listes de colonnes (comme avant) ---
            categorical_cols_freq = ['marital', 'job', 'education', 'month', 'day_of_week', 'poutcome']
            categorical_cols_label = ['default', 'housing', 'loan', 'contact']

            # --- Pr√©traitement ---
            # Encodage par fr√©quence
            try:
                # >>> V√©rification critique de df_original <<<
                if df_original is None:
                    st.error("Erreur critique: df_original non disponible pour calculer les fr√©quences d'encodage.")
                    st.stop() # <<<=== AJOUT CRUCIAL DE st.stop()

                st.write("Application de l'encodage par fr√©quence...")
                for column in categorical_cols_freq:
                     fe = df_original.groupby(column).size() / len(df_original)
                     input_df[f'{column}_freq_encode'] = input_df[column].map(fe).fillna(0)

            except Exception as e:
                 st.error(f"Erreur lors de l'encodage par fr√©quence : {e}")
                 st.stop() # Arr√™ter si l'encodage √©choue

            # Encodage Label
            try:
                st.write("Application de l'encodage Label...")
                le_pred = LabelEncoder()
                for column in categorical_cols_label:
                     all_options = df_original[column].unique() if df_original is not None else locals().get(f"{column}_options", ['yes','no']) # Utiliser df_original ou fallback
                     if not list(all_options): all_options = ['yes', 'no'] # Fallback
                     le_pred.fit(all_options)
                     input_df[column] = le_pred.transform(input_df[column])

            except Exception as e:
                 st.error(f"Erreur lors de l'encodage Label pour {column} : {e}.")
                 st.stop() # Arr√™ter si l'encodage √©choue

            # Supprimer les colonnes originales cat√©gorielles (comme avant)
            cols_to_drop_pred = categorical_cols_freq + categorical_cols_label
            input_df_encoded = input_df.drop(columns=cols_to_drop_pred, errors='ignore')

            # Alignement des colonnes (comme avant)
            try:
                st.write("Alignement des colonnes...")
                input_final = input_df_encoded.reindex(columns=expected_features, fill_value=0)

            except Exception as e:
                 st.error(f"Erreur lors de l'alignement final des colonnes d'entr√©e : {e}")
                 st.stop() # Arr√™ter si l'alignement √©choue

            # --- Pr√©diction ---
            # Ce bloc ne devrait √™tre atteint que si toutes les √©tapes pr√©c√©dentes ont r√©ussi
            try:
                st.write("Lancement de la pr√©diction...")
                prediction_proba = model.predict_proba(input_final) # Assignation de prediction_proba
                prediction = model.predict(input_final) # Assignation de prediction

                # Maintenant, prediction et encoder_y devraient √™tre d√©finis
                result_label = encoder_y.inverse_transform(prediction)[0]
                probability_yes = prediction_proba[0][1]

                # --- Affichage du r√©sultat ---
                st.subheader("R√©sultat de la Pr√©diction")
                if result_label == 'yes':
                    st.success(f"Le client va probablement souscrire ! (Probabilit√©: {probability_yes:.2%})")
                    st.balloons()
                else:
                    st.warning(f"Le client ne va probablement pas souscrire. (Probabilit√© de souscription: {probability_yes:.2%})")

                st.write("Probabilit√©s pr√©dites :")
                st.write(f"- Non ('no'): {prediction_proba[0][0]:.2%}")
                st.write(f"- Oui ('yes'): {prediction_proba[0][1]:.2%}")

            except Exception as e:
                st.error(f"Erreur lors de l'ex√©cution de la pr√©diction : {e}")
                st.write("Donn√©es finales envoy√©es au mod√®le :")
                st.dataframe(input_final)


# --- Reste du code ---
# if __name__ == '__main__':
#     main()
